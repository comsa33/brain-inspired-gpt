#!/usr/bin/env python3
"""
ìì—°ì–´ ì…ë ¥ì„ ë°›ëŠ” CortexGPT ë°ëª¨
ì‹¤ì œë¡œ í•œêµ­ì–´/ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import deque
import numpy as np
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from cortexgpt.models.realtime_cortex import RealTimeCortexGPT, AdvancedMemoryConfig
from cortexgpt.tokenization.multilingual_tokenizer import MultilingualTokenizer
from cortexgpt.learning.realtime_learner import RealTimeLearner


def main():
    print("ğŸ§  CortexGPT ìì—°ì–´ ëŒ€í™” ë°ëª¨")
    print("=" * 50)
    
    # ëª¨ë¸ ì„¤ì •
    config = AdvancedMemoryConfig(
        stm_capacity=64,
        ltm_capacity=1000,
        learning_rate_stm=0.1,
        self_feedback_rate=0.05
    )
    
    # ëª¨ë¸ ìƒì„±
    print("\nğŸ“¦ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = RealTimeCortexGPT(config, vocab_size=10000, dim=256)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    print(f"âœ… {device}ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # í† í¬ë‚˜ì´ì € ìƒì„±
    print("\nğŸ”¤ í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì¤‘...")
    tokenizer = MultilingualTokenizer(vocab_size=10000)
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ
    sample_texts = [
        # í•œêµ­ì–´ ëŒ€í™”
        "ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”",
        "ë„¤, ì •ë§ ì¢‹ì€ ë‚ ì”¨ì˜ˆìš”", "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        "ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì•Œê³  ì‹¶ì–´ìš”", "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„°ê°€ ì¸ê°„ì²˜ëŸ¼ í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤",
        "ê¸°ê³„í•™ìŠµì´ë€ ë¬´ì—‡ì¸ê°€ìš”?", "ê¸°ê³„í•™ìŠµì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ ì°¾ëŠ” ë°©ë²•ì…ë‹ˆë‹¤",
        "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ëŠ”?", "ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤",
        
        # ì˜ì–´ ëŒ€í™”
        "Hello", "Hello, nice to meet you", "How are you today?",
        "I'm doing well, thank you", "What can I help you with?",
        "Tell me about AI", "AI is technology that allows computers to learn like humans",
        "What is machine learning?", "Machine learning is finding patterns from data",
        "What's the difference between deep learning and machine learning?",
        "Deep learning is a subset of machine learning using neural networks",
        
        # ì¼ìƒ ëŒ€í™”
        "ì˜¤ëŠ˜ ë­ ë¨¹ì—ˆì–´ìš”?", "ì ì‹¬ìœ¼ë¡œ ê¹€ì¹˜ì°Œê°œë¥¼ ë¨¹ì—ˆì–´ìš”",
        "What did you have for lunch?", "I had a sandwich",
        "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”", "Have a nice day",
        "ê°ì‚¬í•©ë‹ˆë‹¤", "Thank you", "ì²œë§Œì—ìš”", "You're welcome"
    ] * 10
    
    tokenizer.learn_bpe(sample_texts, verbose=False)
    print(f"âœ… í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ (ì–´íœ˜ í¬ê¸°: {len(tokenizer.vocab)})")
    
    # í•™ìŠµ ì‹œìŠ¤í…œ ì„¤ì •
    print("\nğŸ“ ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ ì‹œì‘...")
    learner = RealTimeLearner(model, tokenizer)
    learner.start()
    print("âœ… í•™ìŠµ ì‹œìŠ¤í…œ í™œì„±í™”")
    
    # ì´ˆê¸° í•™ìŠµ - ê¸°ë³¸ ëŒ€í™” íŒ¨í„´
    print("\nğŸ“š ê¸°ë³¸ ëŒ€í™” íŒ¨í„´ í•™ìŠµ ì¤‘...")
    basic_conversations = [
        ("ì•ˆë…•í•˜ì„¸ìš”", "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤."),
        ("Hello", "Hello! Nice to meet you."),
        ("ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œìš”?", "ì˜¤ëŠ˜ì€ ì¢‹ì€ ë‚ ì”¨ë„¤ìš”."),
        ("How's the weather?", "It's a nice day today."),
        ("ê°ì‚¬í•©ë‹ˆë‹¤", "ì²œë§Œì—ìš”!"),
        ("Thank you", "You're welcome!"),
    ]
    
    for query, expected in basic_conversations:
        response, _ = learner.process_query(query, learn=True)
        print(f"  í•™ìŠµ: {query} â†’ {expected[:20]}...")
    
    print("\nâœ… ê¸°ë³¸ í•™ìŠµ ì™„ë£Œ!")
    
    # ëŒ€í™” ì‹œì‘
    print("\nğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤! (ì¢…ë£Œ: 'quit', ìƒíƒœ: 'stats')")
    print("í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    print("-" * 50)
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            user_input = input("\nğŸ‘¤ ë‹¹ì‹ : ")
            
            if user_input.lower() == 'quit':
                break
            elif user_input.lower() == 'stats':
                show_stats(learner, model)
                continue
            
            # ì‘ë‹µ ìƒì„±
            response, metadata = learner.process_query(user_input, learn=True)
            
            # ì‘ë‹µ í‘œì‹œ
            print(f"ğŸ¤– CortexGPT: {response}")
            
            # ë©”íƒ€ë°ì´í„° í‘œì‹œ
            print(f"   [í’ˆì§ˆ: {metadata['quality_score']:.2f}, "
                  f"ì–¸ì–´: {metadata['language']}, "
                  f"STM: {metadata['confidence']['stm']:.2f}, "
                  f"LTM: {metadata['confidence']['ltm']:.2f}]")
            
            # í•™ìŠµ íš¨ê³¼ í‘œì‹œ
            if metadata['learned']:
                print("   âœ… ì´ ëŒ€í™”ë¡œë¶€í„° í•™ìŠµí–ˆìŠµë‹ˆë‹¤!")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except EOFError:
            print("\n\nâš ï¸ EOF detected - ë¹„ëŒ€í™”í˜• í™˜ê²½ì…ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if "EOF" in str(e):
                break
    
    # ì¢…ë£Œ
    print("\nğŸ›‘ ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
    learner.stop()
    
    # ìµœì¢… í†µê³„
    print("\nğŸ“Š ìµœì¢… ëŒ€í™” í†µê³„")
    print("-" * 40)
    stats = learner.stats
    print(f"ì´ ëŒ€í™” ìˆ˜: {stats['total_queries']}")
    print(f"í•™ìŠµëœ ëŒ€í™”: {stats['total_learned']}")
    print(f"í‰ê·  í’ˆì§ˆ: {stats['avg_quality']:.2f}")
    print(f"ì–¸ì–´ ë¶„í¬: í•œêµ­ì–´={stats['languages']['ko']}, "
          f"ì˜ì–´={stats['languages']['en']}, "
          f"í˜¼í•©={stats['languages']['mixed']}")
    
    print("\nâœ… ë°ëª¨ ì¢…ë£Œ!")
    print("\nCortexGPTì˜ íŠ¹ì§•:")
    print("  â€¢ ëŒ€í™”í•˜ë©´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµ")
    print("  â€¢ ìì£¼ ë‚˜ì˜¤ëŠ” íŒ¨í„´ì€ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì €ì¥")
    print("  â€¢ í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬")
    print("  â€¢ ìŠ¤ìŠ¤ë¡œ ì‘ë‹µì„ í‰ê°€í•˜ê³  ê°œì„ ")


def show_stats(learner, model):
    """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ"""
    print("\nğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
    print("-" * 40)
    
    # í•™ìŠµ í†µê³„
    stats = learner.stats
    print(f"ì´ ëŒ€í™”: {stats['total_queries']}")
    print(f"í•™ìŠµëœ ëŒ€í™”: {stats['total_learned']}")
    print(f"í‰ê·  í’ˆì§ˆ: {stats['avg_quality']:.2f}")
    
    # ì–¸ì–´ ë¶„í¬
    print(f"\nì–¸ì–´ ì‚¬ìš©:")
    total_langs = sum(stats['languages'].values())
    if total_langs > 0:
        for lang, count in stats['languages'].items():
            percentage = (count / total_langs) * 100
            print(f"  {lang}: {count} ({percentage:.1f}%)")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ
    print(f"\në©”ëª¨ë¦¬ ì‚¬ìš©:")
    print(f"  STM: {len(model.stm.memories)} / {model.config.stm_capacity}")
    print(f"  LTM: {len(model.ltm.memories)} / {model.config.ltm_capacity}")
    print(f"  Archive: {model.archive.index.ntotal} / {model.config.archive_capacity}")
    
    # í•™ìŠµë¥ 
    if stats['learning_rate_history']:
        avg_lr = np.mean(list(stats['learning_rate_history']))
        print(f"\ní‰ê·  í•™ìŠµë¥ : {avg_lr:.6f}")


if __name__ == "__main__":
    main()