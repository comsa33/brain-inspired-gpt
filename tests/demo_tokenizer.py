#!/usr/bin/env python3
"""
Demo script to test tokenizer functionality
"""

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from cortexgpt.tokenization.multilingual_tokenizer import MultilingualTokenizer


def demo_tokenizer():
    """Demonstrate tokenizer functionality"""
    
    print("ðŸ”¤ Tokenizer Demo")
    print("=" * 50)
    
    # Create tokenizer
    tokenizer = MultilingualTokenizer(vocab_size=10000)
    
    # Training corpus with diverse text
    training_texts = [
        # English
        "Hello, world! How are you today?",
        "The quick brown fox jumps over the lazy dog.",
        "Machine learning is a subset of artificial intelligence.",
        "Python is a high-level programming language.",
        "def factorial(n): return 1 if n <= 1 else n * factorial(n-1)",
        "Natural language processing enables computers to understand human language.",
        
        # Korean
        "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.",
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ëž˜ì˜ í•µì‹¬ ê¸°ìˆ ìž…ë‹ˆë‹¤.",
        "í•œêµ­ì–´ ìžì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì—°êµ¬ ë¶„ì•¼ìž…ë‹ˆë‹¤.",
        "íŒŒì´ì¬ì€ ë°°ìš°ê¸° ì‰¬ìš´ í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ìž…ë‹ˆë‹¤.",
        "ê¸°ê³„ í•™ìŠµì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ê¸°ê³„ í•™ìŠµì˜ í•œ ë¶„ì•¼ìž…ë‹ˆë‹¤.",
        
        # Mixed
        "AI (ì¸ê³µì§€ëŠ¥) is changing the world.",
        "Pythonê³¼ JavaScriptëŠ” ì¸ê¸° ìžˆëŠ” í”„ë¡œê·¸ëž˜ë° ì–¸ì–´ìž…ë‹ˆë‹¤.",
        "Machine Learningê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ] * 10  # Repeat for better BPE learning
    
    print(f"Training tokenizer on {len(training_texts)} texts...")
    tokenizer.learn_bpe(training_texts, verbose=True)
    
    print(f"\nâœ… Tokenizer created with vocabulary size: {len(tokenizer.vocab)}")
    
    # Show special tokens
    print("\nðŸ“Œ Special tokens:")
    for name, token_id in tokenizer.special_tokens.items():
        print(f"  {name}: {token_id}")
    
    # Test tokenization
    test_texts = [
        "Hello, world!",
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "Machine learning is amazing.",
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ëž˜ìž…ë‹ˆë‹¤.",
        "def hello(): print('world')",
    ]
    
    print("\nðŸ§ª Tokenization tests:")
    print("-" * 50)
    
    for text in test_texts:
        tokens = tokenizer.encode(text)
        decoded = tokenizer.decode(tokens)
        
        print(f"\nOriginal: {text}")
        print(f"Tokens: {tokens[:20]}{'...' if len(tokens) > 20 else ''}")
        print(f"Decoded: {decoded}")
        print(f"Token count: {len(tokens)}")
        
        # Show individual tokens
        if len(tokens) <= 15:
            token_strs = []
            for token_id in tokens:
                # Get token string from reverse_vocab mapping
                token_str = tokenizer.reverse_vocab.get(token_id, f"<UNK:{token_id}>")
                if isinstance(token_str, str) and token_str.startswith('##'):
                    token_str = token_str[2:]  # Remove ## prefix
                token_strs.append(repr(token_str))
            print(f"Token strings: {' '.join(token_strs)}")
    
    # Check vocabulary coverage
    print("\nðŸ“Š Vocabulary analysis:")
    
    # Count token types
    special_count = sum(1 for token in tokenizer.vocab.keys() if token.startswith('<') and token.endswith('>'))
    korean_count = sum(1 for token in tokenizer.vocab.keys() if any('\uac00' <= c <= '\ud7af' for c in token))
    english_count = sum(1 for token in tokenizer.vocab.keys() if token.isalpha() and all(c < '\u0080' for c in token))
    
    print(f"  Total vocabulary: {len(tokenizer.vocab)}")
    print(f"  Special tokens: {special_count}")
    print(f"  Korean tokens: {korean_count}")
    print(f"  English tokens: {english_count}")
    print(f"  Other tokens: {len(tokenizer.vocab) - special_count - korean_count - english_count}")
    
    # Test unknown token handling
    print("\nðŸ” Unknown token handling:")
    unknown_text = "è¿™æ˜¯ä¸­æ–‡æ–‡æœ¬ ðŸš€ Emoji test!"
    tokens = tokenizer.encode(unknown_text)
    decoded = tokenizer.decode(tokens)
    print(f"Original: {unknown_text}")
    print(f"Decoded: {decoded}")
    print(f"Contains <unk>: {'<unk>' in decoded}")


if __name__ == "__main__":
    demo_tokenizer()