#!/usr/bin/env python3
"""
Quick validation script for Brain-Inspired GPT
Checks if everything is working correctly
"""

import sys
import os
import torch

# Add brain_gpt to path
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'brain_gpt'))

# Initialize test results
test_results = {
    'imports': False,
    'model_creation': False,
    'cuda': False,
    'tokenizer': False,
    'forward_pass': False,
    'generation': False
}

print("üß† Brain-Inspired GPT Validation")
print("="*50)

# Test 1: Imports
print("\n1Ô∏è‚É£ Testing imports...")
try:
    from core.model_brain import BrainGPT
    from core.model_brain_config import BrainGPTConfig
    from core.multilingual_tokenizer import MultilingualBrainTokenizer
    print("‚úÖ All imports successful")
    test_results['imports'] = True
except Exception as e:
    print(f"‚ùå Import error: {e}")
    sys.exit(1)

# Test 2: Model creation
print("\n2Ô∏è‚É£ Testing model creation...")
try:
    config = BrainGPTConfig()
    config.n_layer = 2  # Small for testing
    config.n_embd = 256
    # Ensure cortical columns match embedding size for testing
    config.n_cortical_columns = 8
    config.column_size = 32  # 8 * 32 = 256 = n_embd
    
    model = BrainGPT(config)
    params = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ Model created: {params/1e6:.1f}M parameters")
    test_results['model_creation'] = True
except Exception as e:
    print(f"‚ùå Model creation error: {e}")
    sys.exit(1)

# Test 3: CUDA availability
print("\n3Ô∏è‚É£ Testing CUDA...")
if torch.cuda.is_available():
    print(f"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}")
    model = model.cuda()
    print("‚úÖ Model moved to GPU")
    test_results['cuda'] = True
else:
    print("‚ö†Ô∏è  CUDA not available, using CPU")
    test_results['cuda'] = False

# Test 4: Tokenizer
print("\n4Ô∏è‚É£ Testing tokenizer...")
try:
    tokenizer = MultilingualBrainTokenizer()
    
    # Test English
    text_en = "Hello world"
    tokens_en = tokenizer.encode(text_en, language='en')
    print(f"‚úÖ English tokenization: '{text_en}' -> {len(tokens_en)} tokens")
    
    # Test Korean
    text_ko = "ÏïàÎÖïÌïòÏÑ∏Ïöî"
    tokens_ko = tokenizer.encode(text_ko, language='ko')
    print(f"‚úÖ Korean tokenization: '{text_ko}' -> {len(tokens_ko)} tokens")
    test_results['tokenizer'] = True
except Exception as e:
    print(f"‚ùå Tokenizer error: {e}")
    import traceback
    traceback.print_exc()
    test_results['tokenizer'] = False

# Test 5: Forward pass
print("\n5Ô∏è‚É£ Testing forward pass...")
try:
    input_ids = torch.randint(0, config.vocab_size, (1, 32))
    if torch.cuda.is_available():
        input_ids = input_ids.cuda()
    
    with torch.no_grad():
        output = model(input_ids)
        if isinstance(output, tuple):
            output = output[0]
    
    print(f"‚úÖ Forward pass successful: output shape {output.shape}")
    test_results['forward_pass'] = True
except Exception as e:
    print(f"‚ùå Forward pass error: {e}")
    import traceback
    traceback.print_exc()
    test_results['forward_pass'] = False

# Test 6: Generation
print("\n6Ô∏è‚É£ Testing generation...")
try:
    prompt = "The future is"
    tokens = tokenizer.encode(prompt)[:20]  # Limit length
    input_ids = torch.tensor(tokens).unsqueeze(0)
    
    if torch.cuda.is_available():
        input_ids = input_ids.cuda()
    
    with torch.no_grad():
        output_ids = model.generate(
            input_ids,
            max_new_tokens=10,
            temperature=0.8
        )
    
    print(f"‚úÖ Generation successful: {len(output_ids[0])} total tokens")
    test_results['generation'] = True
except Exception as e:
    print(f"‚ùå Generation error: {e}")
    import traceback
    traceback.print_exc()
    test_results['generation'] = False

# Test results are already initialized at the beginning of the file

# Summary
print("\n" + "="*50)
print("üìä VALIDATION SUMMARY")
print("="*50)

passed = sum(test_results.values())
total = len(test_results)
print(f"\nPassed: {passed}/{total} tests")

if passed < total:
    print("\n‚ö†Ô∏è  Some tests failed. This is normal for initial setup.")
    print("The core model is working, but some features need configuration.")

print("""
‚úÖ Core functionality is working!

üöÄ Next steps:
   1. Run the interactive demo:
      uv run brain_gpt/quickstart_brain_gpt.py
      
   2. Train a model:
      uv run brain_gpt/training/train_brain_gpt.py
      
   3. Run comprehensive tests:
      uv run brain_gpt/tests/run_all_tests.py
      
   4. See documentation:
      brain_gpt/docs/README_BRAIN_GPT.md
""")

print("üéâ Brain-Inspired GPT is ready to use!")