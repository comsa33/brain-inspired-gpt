#!/usr/bin/env python3
"""
Comprehensive test suite for Brain-Inspired GPT
Tests all major functions: data loading, training, inference, and performance
"""

import os
import sys
import torch
import pytest
import tempfile
import time
from pathlib import Path

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.model_brain import BrainGPT
from core.model_brain_config import BrainGPTConfig
from core.multilingual_tokenizer import MultilingualBrainTokenizer, KoreanDataCollator
from core.sparse_modules import CorticalColumnLinear, SpikingCorticalColumn
from training.prepare_korean_data import KoreanDatasetPreprocessor


class TestBrainGPT:
    """Test suite for Brain-Inspired GPT"""
    
    @pytest.fixture
    def config(self):
        """Create test configuration"""
        config = BrainGPTConfig()
        config.n_layer = 4  # Smaller for testing
        config.n_embd = 512
        config.n_head = 8
        config.block_size = 128
        return config
    
    @pytest.fixture
    def model(self, config):
        """Create test model"""
        return BrainGPT(config)
    
    @pytest.fixture
    def tokenizer(self):
        """Create tokenizer"""
        return MultilingualBrainTokenizer(cache_dir="./test_tokenizer_cache")
    
    def test_model_initialization(self, model, config):
        """Test model initialization and parameter count"""
        print("\nğŸ§ª Testing Model Initialization...")
        
        # Check model structure
        assert hasattr(model, 'transformer')
        assert hasattr(model, 'lm_head')
        assert len(model.transformer.h) == config.n_layer
        
        # Check parameter count
        total_params = sum(p.numel() for p in model.parameters())
        print(f"âœ… Model initialized with {total_params/1e6:.1f}M parameters")
        assert total_params > 0
        
        # Check sparsity
        effective_params = total_params * (1 - config.sparsity_base)
        print(f"âœ… Effective parameters (with sparsity): {effective_params/1e6:.1f}M")
        
    def test_tokenizer_basic(self, tokenizer):
        """Test basic tokenizer functionality"""
        print("\nğŸ§ª Testing Tokenizer...")
        
        # Test English
        text_en = "Hello, world!"
        tokens_en = tokenizer.encode(text_en, language='en')
        decoded_en = tokenizer.decode(tokens_en)
        print(f"âœ… English: '{text_en}' -> {len(tokens_en)} tokens")
        assert len(tokens_en) > 0
        
        # Test Korean
        text_ko = "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ê³„!"
        tokens_ko = tokenizer.encode(text_ko, language='ko')
        decoded_ko = tokenizer.decode(tokens_ko)
        print(f"âœ… Korean: '{text_ko}' -> {len(tokens_ko)} tokens")
        assert len(tokens_ko) > 0
        
        # Test mixed
        text_mixed = "Hello ì•ˆë…•í•˜ì„¸ìš”!"
        tokens_mixed = tokenizer.encode(text_mixed)
        decoded_mixed = tokenizer.decode(tokens_mixed)
        print(f"âœ… Mixed: '{text_mixed}' -> {len(tokens_mixed)} tokens")
        assert len(tokens_mixed) > 0
        
    def test_language_detection(self, tokenizer):
        """Test language detection"""
        print("\nğŸ§ª Testing Language Detection...")
        
        test_cases = [
            ("Hello world", "en"),
            ("ì•ˆë…•í•˜ì„¸ìš”", "ko"),
            ("def hello():", "code"),
            ("AIì™€ machine learning", "ko"),  # Mixed but predominantly Korean
        ]
        
        for text, expected_lang in test_cases:
            detected = tokenizer.detect_language(text)
            print(f"âœ… '{text}' detected as: {detected}")
            assert detected == expected_lang
            
    def test_forward_pass(self, model, tokenizer, config):
        """Test model forward pass"""
        print("\nğŸ§ª Testing Forward Pass...")
        
        # Create sample input
        text = "The future of AI is"
        tokens = tokenizer.encode(text)[:config.block_size]
        input_ids = torch.tensor(tokens).unsqueeze(0)
        
        # Forward pass
        device = next(model.parameters()).device
        input_ids = input_ids.to(device)
        
        with torch.no_grad():
            output = model(input_ids)
            
        assert output.shape == (1, len(tokens), config.vocab_size)
        print(f"âœ… Forward pass successful: {output.shape}")
        
    def test_generation(self, model, tokenizer, config):
        """Test text generation"""
        print("\nğŸ§ª Testing Text Generation...")
        
        device = next(model.parameters()).device
        model.eval()
        
        prompts = [
            ("The future of AI", "en"),
            ("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜", "ko"),
        ]
        
        for prompt, lang in prompts:
            tokens = tokenizer.encode(prompt, language=lang)
            input_ids = torch.tensor(tokens).unsqueeze(0).to(device)
            
            with torch.no_grad():
                output = model.generate(
                    input_ids,
                    max_new_tokens=20,
                    temperature=0.8,
                    language_id=lang
                )
                
            generated = tokenizer.decode(output[0].tolist())
            print(f"âœ… Generated from '{prompt}': {len(output[0])} tokens")
            assert len(output[0]) > len(tokens)
            
    def test_sparse_modules(self):
        """Test sparse module functionality"""
        print("\nğŸ§ª Testing Sparse Modules...")
        
        # Test CorticalColumnLinear
        linear = CorticalColumnLinear(512, 256, num_columns=8)
        x = torch.randn(1, 10, 512)
        output = linear(x)
        assert output.shape == (1, 10, 256)
        print("âœ… CorticalColumnLinear forward pass successful")
        
        # Check sparsity
        sparsity = linear.get_sparsity_stats()
        print(f"âœ… Sparsity: {sparsity['sparsity']:.1%}")
        assert sparsity['sparsity'] > 0.5
        
    def test_data_collator(self, tokenizer):
        """Test Korean data collator"""
        print("\nğŸ§ª Testing Data Collator...")
        
        collator = KoreanDataCollator(tokenizer, max_length=128)
        
        examples = [
            {"text": "Hello world", "language": "en"},
            {"text": "ì•ˆë…•í•˜ì„¸ìš”", "language": "ko"},
        ]
        
        batch = collator(examples)
        
        assert 'input_ids' in batch
        assert 'attention_mask' in batch
        assert 'language_ids' in batch
        assert batch['input_ids'].shape == (2, 128)
        print("âœ… Data collator working correctly")
        
    def test_memory_efficiency(self, model, config):
        """Test memory efficiency"""
        print("\nğŸ§ª Testing Memory Efficiency...")
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            model = model.cuda()
            
            # Run forward pass
            input_ids = torch.randint(0, config.vocab_size, (1, config.block_size)).cuda()
            
            with torch.no_grad():
                output = model(input_ids)
                
            memory_used = torch.cuda.max_memory_allocated() / 1e9
            print(f"âœ… Peak memory usage: {memory_used:.2f} GB")
            
            # Check if memory usage is reasonable
            expected_memory = (sum(p.numel() for p in model.parameters()) * 4) / 1e9  # 4 bytes per param
            efficiency = expected_memory / memory_used
            print(f"âœ… Memory efficiency: {efficiency:.1%}")
        else:
            print("âš ï¸  CUDA not available, skipping memory test")
            
    def test_training_step(self, model, tokenizer, config):
        """Test single training step"""
        print("\nğŸ§ª Testing Training Step...")
        
        device = next(model.parameters()).device
        model.train()
        
        # Create batch
        texts = ["The future is", "AI will"]
        batch_tokens = []
        for text in texts:
            tokens = tokenizer.encode(text)[:config.block_size]
            tokens += [0] * (config.block_size - len(tokens))  # Pad
            batch_tokens.append(tokens)
            
        input_ids = torch.tensor(batch_tokens).to(device)
        targets = input_ids.clone()
        
        # Forward pass
        logits = model(input_ids)
        
        # Compute loss
        loss = torch.nn.functional.cross_entropy(
            logits.view(-1, logits.size(-1)),
            targets.view(-1)
        )
        
        assert loss.item() > 0
        print(f"âœ… Training loss: {loss.item():.4f}")
        
        # Backward pass
        loss.backward()
        
        # Check gradients
        has_grads = any(p.grad is not None and p.grad.abs().sum() > 0 
                       for p in model.parameters() if p.requires_grad)
        assert has_grads
        print("âœ… Gradients computed successfully")
        
    @pytest.mark.slow
    def test_performance_benchmarks(self, model, tokenizer, config):
        """Test performance benchmarks"""
        print("\nğŸ§ª Testing Performance Benchmarks...")
        
        if not torch.cuda.is_available():
            print("âš ï¸  CUDA not available, skipping performance test")
            return
            
        model = model.cuda().eval()
        
        # Warm up
        input_ids = torch.randint(0, config.vocab_size, (1, config.block_size)).cuda()
        for _ in range(10):
            with torch.no_grad():
                _ = model(input_ids)
                
        # Benchmark
        num_iterations = 100
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(num_iterations):
            with torch.no_grad():
                _ = model(input_ids)
                
        torch.cuda.synchronize()
        total_time = time.time() - start_time
        
        tokens_per_second = (num_iterations * config.block_size) / total_time
        print(f"âœ… Throughput: {tokens_per_second:.0f} tokens/sec")
        print(f"âœ… Latency: {total_time/num_iterations*1000:.1f} ms/batch")
        
    def test_korean_data_preprocessing(self):
        """Test Korean data preprocessing"""
        print("\nğŸ§ª Testing Korean Data Preprocessing...")
        
        preprocessor = KoreanDatasetPreprocessor()
        
        # Test text cleaning
        dirty_text = "ì•ˆë…•í•˜ì„¸ìš”!!!   ì„¸ê³„~~~ ã…‹ã…‹ã…‹"
        clean_text = preprocessor.clean_text(dirty_text)
        print(f"âœ… Cleaned: '{dirty_text}' -> '{clean_text}'")
        
        # Test sentence splitting
        text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”! AIëŠ” ë¯¸ë˜ì…ë‹ˆë‹¤."
        sentences = preprocessor.split_sentences(text)
        print(f"âœ… Split into {len(sentences)} sentences")
        assert len(sentences) == 3
        
    def test_end_to_end_pipeline(self, config):
        """Test complete pipeline from data to generation"""
        print("\nğŸ§ª Testing End-to-End Pipeline...")
        
        # Initialize components
        tokenizer = MultilingualBrainTokenizer()
        model = BrainGPT(config)
        
        if torch.cuda.is_available():
            model = model.cuda()
            
        # Prepare data
        train_texts = [
            "Artificial intelligence is transforming the world.",
            "ì¸ê³µì§€ëŠ¥ì€ ì„¸ìƒì„ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.",
            "def hello_world():\n    print('Hello!')",
        ]
        
        # Train for a few steps
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
        
        for i, text in enumerate(train_texts * 2):
            tokens = tokenizer.encode(text)[:config.block_size]
            if len(tokens) < 2:
                continue
                
            input_ids = torch.tensor(tokens[:-1]).unsqueeze(0)
            targets = torch.tensor(tokens[1:]).unsqueeze(0)
            
            if torch.cuda.is_available():
                input_ids = input_ids.cuda()
                targets = targets.cuda()
                
            logits = model(input_ids)
            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1)
            )
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if i % 3 == 0:
                print(f"âœ… Step {i}: Loss = {loss.item():.4f}")
                
        # Test generation after training
        model.eval()
        test_prompt = "AI is"
        tokens = tokenizer.encode(test_prompt)
        input_ids = torch.tensor(tokens).unsqueeze(0)
        
        if torch.cuda.is_available():
            input_ids = input_ids.cuda()
            
        with torch.no_grad():
            output = model.generate(input_ids, max_new_tokens=10)
            
        generated = tokenizer.decode(output[0].tolist())
        print(f"âœ… Generated: '{generated}'")
        
        print("\nâœ… End-to-end pipeline test completed successfully!")


def run_all_tests():
    """Run all tests with detailed output"""
    print("ğŸ§  Brain-Inspired GPT Comprehensive Test Suite")
    print("=" * 60)
    
    # Run pytest with verbose output
    pytest.main([__file__, "-v", "-s"])


if __name__ == "__main__":
    run_all_tests()