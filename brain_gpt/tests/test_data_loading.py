#!/usr/bin/env python3
"""
Test data loading functionality for Brain-Inspired GPT
Tests Korean dataset preparation, tokenization, and data pipeline
"""

import os
import sys
import torch
from torch.utils.data import DataLoader
import tempfile
import json

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.multilingual_tokenizer import MultilingualBrainTokenizer, KoreanDataCollator
from training.prepare_korean_data import KoreanDatasetPreprocessor


def test_korean_tokenizer():
    """Test Korean tokenizer functionality"""
    print("\nğŸ§ª Testing Korean Tokenizer...")
    
    tokenizer = MultilingualBrainTokenizer()
    
    # Test cases
    test_texts = [
        ("ì•ˆë…•í•˜ì„¸ìš”", "ko", "Basic Korean greeting"),
        ("ì¸ê³µì§€ëŠ¥", "ko", "Korean AI term"),
        ("ì„œìš¸íŠ¹ë³„ì‹œ", "ko", "Seoul city name"),
        ("ê°ì‚¬í•©ë‹ˆë‹¤", "ko", "Korean thanks"),
        ("2024ë…„ 1ì›”", "ko", "Korean date"),
        ("AIì™€ ë¨¸ì‹ ëŸ¬ë‹", "ko", "Mixed Korean-English"),
    ]
    
    for text, expected_lang, description in test_texts:
        # Test language detection
        detected_lang = tokenizer.detect_language(text)
        print(f"\nğŸ“ {description}: '{text}'")
        print(f"   Detected language: {detected_lang}")
        
        # Test encoding
        tokens = tokenizer.encode(text, language=expected_lang)
        print(f"   Encoded to {len(tokens)} tokens")
        
        # Test decoding
        decoded = tokenizer.decode(tokens)
        print(f"   Decoded: '{decoded}'")
        
        # Verify round-trip
        if text in decoded or decoded in text:
            print("   âœ… Round-trip successful")
        else:
            print("   âš ï¸  Round-trip mismatch")
            
            
def test_data_preparation():
    """Test Korean data preparation"""
    print("\nğŸ§ª Testing Data Preparation...")
    
    # Create tokenizer first
    tokenizer = MultilingualBrainTokenizer()
    preprocessor = KoreanDatasetPreprocessor(tokenizer)
    
    # Create sample Korean data
    sample_data = [
        "ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¼ê¹Œìš”?",
        "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ëŠ” ë§¤ìš° í¥ë¯¸ë¡œìš´ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "ì„œìš¸ì—ì„œ ë¶€ì‚°ê¹Œì§€ KTXë¡œ 2ì‹œê°„ 30ë¶„ì´ ê±¸ë¦½ë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”!",
    ]
    
    # Test text processing
    processed_data = []
    for text in sample_data:
        # Clean text
        cleaned = preprocessor.clean_text(text)
        
        # Split into sentences
        sentences = preprocessor.split_sentences(cleaned)
        
        print(f"\nì›ë¬¸: {text}")
        print(f"ì •ì œ: {cleaned}")
        print(f"ë¬¸ì¥ ìˆ˜: {len(sentences)}")
        
        processed_data.extend(sentences)
        
    print(f"\nâœ… Processed {len(sample_data)} texts into {len(processed_data)} sentences")
    
    return processed_data


def test_data_collator():
    """Test data collation for training"""
    print("\nğŸ§ª Testing Data Collator...")
    
    tokenizer = MultilingualBrainTokenizer()
    collator = KoreanDataCollator(tokenizer, max_length=64)
    
    # Create sample batch
    examples = [
        {"text": "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ë°ìŠµë‹ˆë‹¤.", "language": "ko"},
        {"text": "The future of AI is bright.", "language": "en"},
        {"text": "def train_model(data):", "language": "code"},
        {"text": "ì„œìš¸ì€ í•œêµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤.", "language": "ko"},
    ]
    
    # Collate batch
    batch = collator(examples)
    
    print(f"\nBatch shapes:")
    print(f"  input_ids: {batch['input_ids'].shape}")
    print(f"  attention_mask: {batch['attention_mask'].shape}")
    print(f"  language_ids: {batch['language_ids'].shape}")
    print(f"  labels: {batch['labels'].shape}")
    
    # Verify batch content
    for i, example in enumerate(examples):
        lang_id = batch['language_ids'][i].item()
        lang_map = {0: 'en', 1: 'ko', 2: 'code'}
        print(f"\n  Example {i}: {example['language']} -> {lang_map.get(lang_id, 'unknown')}")
        
    print("\nâœ… Data collator working correctly")
    
    return batch


def test_dataloader():
    """Test PyTorch DataLoader integration"""
    print("\nğŸ§ª Testing DataLoader...")
    
    tokenizer = MultilingualBrainTokenizer()
    collator = KoreanDataCollator(tokenizer, max_length=64)
    
    # Create dataset
    dataset = [
        {"text": f"í•œêµ­ì–´ ë¬¸ì¥ {i}ë²ˆì…ë‹ˆë‹¤.", "language": "ko"}
        for i in range(20)
    ]
    dataset.extend([
        {"text": f"English sentence number {i}.", "language": "en"}
        for i in range(20)
    ])
    
    # Create DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        collate_fn=collator
    )
    
    # Test iteration
    print(f"\nDataset size: {len(dataset)}")
    print(f"Batch size: 4")
    print(f"Expected batches: {len(dataset) // 4}")
    
    batch_count = 0
    total_samples = 0
    
    for batch in dataloader:
        batch_count += 1
        batch_size = batch['input_ids'].shape[0]
        total_samples += batch_size
        
        if batch_count == 1:
            print(f"\nFirst batch:")
            print(f"  Batch size: {batch_size}")
            print(f"  Input shape: {batch['input_ids'].shape}")
            print(f"  Languages: {batch['language_ids'].tolist()}")
            
    print(f"\nâœ… Processed {batch_count} batches, {total_samples} total samples")
    

def test_save_load_data():
    """Test saving and loading processed data"""
    print("\nğŸ§ª Testing Data Save/Load...")
    
    # Create temporary directory
    with tempfile.TemporaryDirectory() as temp_dir:
        # Prepare some data
        data = [
            {"text": "ì•ˆë…•í•˜ì„¸ìš”, ì¸ê³µì§€ëŠ¥ì…ë‹ˆë‹¤.", "language": "ko"},
            {"text": "Hello, I am AI.", "language": "en"},
        ]
        
        # Save as JSON
        json_path = os.path.join(temp_dir, "test_data.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… Saved {len(data)} examples to JSON")
        
        # Load back
        with open(json_path, 'r', encoding='utf-8') as f:
            loaded_data = json.load(f)
            
        print(f"âœ… Loaded {len(loaded_data)} examples from JSON")
        
        # Verify
        assert len(loaded_data) == len(data)
        assert loaded_data[0]['text'] == data[0]['text']
        print("âœ… Data integrity verified")
        

def test_mixed_language_handling():
    """Test handling of mixed language content"""
    print("\nğŸ§ª Testing Mixed Language Handling...")
    
    tokenizer = MultilingualBrainTokenizer()
    
    mixed_texts = [
        "AIì™€ ì¸ê³µì§€ëŠ¥ì€ ê°™ì€ meaningì…ë‹ˆë‹¤.",
        "Deep learningì€ ë”¥ëŸ¬ë‹ìœ¼ë¡œ ë²ˆì—­ë©ë‹ˆë‹¤.",
        "Pythonìœ¼ë¡œ AI ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤.",
        "ì„œìš¸ Seoul ëŒ€í•œë¯¼êµ­ Korea",
    ]
    
    for text in mixed_texts:
        print(f"\ní…ìŠ¤íŠ¸: '{text}'")
        
        # Detect chunks
        chunks = tokenizer._split_by_language(text)
        print(f"ì²­í¬ ìˆ˜: {len(chunks)}")
        
        for chunk_text, chunk_lang in chunks:
            print(f"  [{chunk_lang}] '{chunk_text}'")
            
        # Encode
        tokens = tokenizer.encode(text)
        print(f"í† í° ìˆ˜: {len(tokens)}")
        
        # Decode
        decoded = tokenizer.decode(tokens)
        print(f"ë””ì½”ë”©: '{decoded}'")
        

def run_all_data_tests():
    """Run all data loading tests"""
    print("ğŸ§  Brain-Inspired GPT Data Loading Tests")
    print("=" * 60)
    
    try:
        # Run tests
        test_korean_tokenizer()
        processed_data = test_data_preparation()
        batch = test_data_collator()
        test_dataloader()
        test_save_load_data()
        test_mixed_language_handling()
        
        print("\n" + "=" * 60)
        print("âœ… All data loading tests passed!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        

if __name__ == "__main__":
    run_all_data_tests()