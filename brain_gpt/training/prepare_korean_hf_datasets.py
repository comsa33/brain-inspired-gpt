#!/usr/bin/env python3
"""
Prepare Korean datasets from Hugging Face for Brain-Inspired GPT
Uses freely available Korean datasets without requiring registration
"""

import os
import sys
import json
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Optional

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.multilingual_tokenizer import MultilingualBrainTokenizer


class KoreanHFDatasetPreparer:
    """Prepare Korean datasets from Hugging Face"""
    
    def __init__(self, data_dir: str = "data/korean_hf"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.tokenizer = MultilingualBrainTokenizer()
        
    def load_klue_datasets(self) -> Dict[str, List[str]]:
        """Load KLUE datasets and extract text"""
        print("\nğŸ“¥ Loading KLUE datasets from Hugging Face...")
        
        try:
            from datasets import load_dataset
            
            all_texts = []
            
            # KLUE-TC (Topic Classification) - contains news articles
            try:
                print("  Loading KLUE-TC (News articles)...")
                dataset = load_dataset("klue", "ynat")
                for split in ['train', 'validation']:
                    for item in dataset[split]:
                        title = item.get('title', '')
                        # YNAT dataset has news titles
                        if title:
                            all_texts.append(title)
                print(f"  âœ… Loaded {len(all_texts)} news titles")
            except Exception as e:
                print(f"  âš ï¸  Failed to load KLUE-TC: {e}")
                
            # KLUE-STS (Semantic Textual Similarity) - sentence pairs
            try:
                print("  Loading KLUE-STS (Sentence pairs)...")
                sts_texts = []
                dataset = load_dataset("klue", "sts")
                for split in ['train', 'validation']:
                    for item in dataset[split]:
                        sts_texts.append(item['sentence1'])
                        sts_texts.append(item['sentence2'])
                all_texts.extend(sts_texts)
                print(f"  âœ… Loaded {len(sts_texts)} sentences from STS")
            except Exception as e:
                print(f"  âš ï¸  Failed to load KLUE-STS: {e}")
                
            # KLUE-NLI (Natural Language Inference)
            try:
                print("  Loading KLUE-NLI (Premise-hypothesis pairs)...")
                nli_texts = []
                dataset = load_dataset("klue", "nli")
                for split in ['train', 'validation']:
                    for item in dataset[split]:
                        nli_texts.append(item['premise'])
                        nli_texts.append(item['hypothesis'])
                all_texts.extend(nli_texts)
                print(f"  âœ… Loaded {len(nli_texts)} sentences from NLI")
            except Exception as e:
                print(f"  âš ï¸  Failed to load KLUE-NLI: {e}")
                
            return {'klue': all_texts}
            
        except ImportError:
            print("âŒ datasets library not found. Install with: pip install datasets")
            return {}
            
    def load_korquad_dataset(self) -> Dict[str, List[str]]:
        """Load KorQuAD dataset"""
        print("\nğŸ“¥ Loading KorQuAD dataset...")
        
        try:
            from datasets import load_dataset
            
            all_texts = []
            
            # Load KorQuAD v1.0
            dataset = load_dataset("squad_kor_v1")
            
            for split in ['train', 'validation']:
                for item in dataset[split]:
                    # Extract context (passage)
                    context = item.get('context', '')
                    if context:
                        all_texts.append(context)
                        
                    # Also add questions and answers for variety
                    question = item.get('question', '')
                    if question:
                        all_texts.append(question)
                        
                    answers = item.get('answers', {})
                    if answers and 'text' in answers:
                        for answer in answers['text']:
                            if answer:
                                all_texts.append(answer)
                                
            print(f"âœ… Loaded {len(all_texts)} texts from KorQuAD")
            return {'korquad': all_texts}
            
        except Exception as e:
            print(f"âŒ Failed to load KorQuAD: {e}")
            return {}
            
    def load_nsmc_dataset(self) -> Dict[str, List[str]]:
        """Load NSMC (Naver Sentiment Movie Corpus)"""
        print("\nğŸ“¥ Loading NSMC dataset...")
        
        try:
            from datasets import load_dataset
            
            all_texts = []
            
            # Load NSMC
            dataset = load_dataset("nsmc")
            
            for split in ['train', 'test']:
                for item in dataset[split]:
                    document = item.get('document', '')
                    if document:
                        all_texts.append(document)
                        
            print(f"âœ… Loaded {len(all_texts)} movie reviews from NSMC")
            return {'nsmc': all_texts}
            
        except Exception as e:
            print(f"âŒ Failed to load NSMC: {e}")
            return {}
            
    def load_korean_parallel_corpora(self) -> Dict[str, List[str]]:
        """Load Korean parallel corpora"""
        print("\nğŸ“¥ Loading Korean parallel corpora...")
        
        try:
            from datasets import load_dataset
            
            all_texts = []
            
            # Try to load Korean-English parallel data
            try:
                dataset = load_dataset("Helsinki-NLP/opus-100", "en-ko")
                for split in ['train', 'validation', 'test']:
                    if split in dataset:
                        for item in dataset[split]:
                            ko_text = item['translation']['ko']
                            if ko_text:
                                all_texts.append(ko_text)
                print(f"âœ… Loaded {len(all_texts)} Korean texts from parallel corpus")
            except Exception as e:
                print(f"âš ï¸  Failed to load parallel corpus: {e}")
                
            return {'parallel': all_texts}
            
        except Exception as e:
            print(f"âŒ Failed to load parallel corpora: {e}")
            return {}
            
    def create_high_quality_samples(self) -> List[str]:
        """Create high-quality Korean text samples for training"""
        print("\nğŸ“ Creating high-quality Korean samples...")
        
        samples = [
            # Technology and AI
            "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ ë°œì „ì€ ìš°ë¦¬ ì‚¬íšŒì— í˜ëª…ì ì¸ ë³€í™”ë¥¼ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œì˜ ë°œì „ì€ ì¸ê°„ê³¼ ê¸°ê³„ ê°„ì˜ ì†Œí†µì„ ë”ìš± ì›í™œí•˜ê²Œ ë§Œë“¤ê³  ìˆìŠµë‹ˆë‹¤.",
            "ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ë°œì „ìœ¼ë¡œ ì»´í“¨í„° ë¹„ì „, ìŒì„± ì¸ì‹, ìì—°ì–´ ì´í•´ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "í•œêµ­ì€ 5G ë„¤íŠ¸ì›Œí¬ ê¸°ìˆ ì„ ì„¸ê³„ ìµœì´ˆë¡œ ìƒìš©í™”í•˜ì—¬ ì •ë³´í†µì‹  ë¶„ì•¼ì˜ ì„ ë„êµ­ê°€ë¡œ ìë¦¬ë§¤ê¹€í–ˆìŠµë‹ˆë‹¤.",
            
            # Korean culture and history
            "í•œê¸€ì€ ì„¸ì¢…ëŒ€ì™•ì´ 1443ë…„ì— ì°½ì œí•œ ê³¼í•™ì ì´ê³  ì²´ê³„ì ì¸ ë¬¸ì ì²´ê³„ë¡œ, ìœ ë„¤ìŠ¤ì½” ì„¸ê³„ê¸°ë¡ìœ ì‚°ì— ë“±ì¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
            "í•œêµ­ì˜ ì „í†µ ìŒì‹ì¸ ê¹€ì¹˜ëŠ” ë°œíš¨ ê³¼í•™ì˜ ì •ìˆ˜ë¥¼ ë³´ì—¬ì£¼ëŠ” ê±´ê°•ì‹í’ˆìœ¼ë¡œ, ì„¸ê³„ì ìœ¼ë¡œ ê·¸ ê°€ì¹˜ë¥¼ ì¸ì •ë°›ê³  ìˆìŠµë‹ˆë‹¤.",
            "K-popê³¼ K-dramaë¡œ ëŒ€í‘œë˜ëŠ” í•œë¥˜ëŠ” ì „ ì„¸ê³„ì— í•œêµ­ ë¬¸í™”ë¥¼ ì•Œë¦¬ëŠ” ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            
            # Science and education
            "í•œêµ­ì˜ êµìœ¡ ì‹œìŠ¤í…œì€ ë†’ì€ í•™ì—… ì„±ì·¨ë„ë¡œ ìœ ëª…í•˜ë©°, íŠ¹íˆ ìˆ˜í•™ê³¼ ê³¼í•™ ë¶„ì•¼ì—ì„œ ì„¸ê³„ì ì¸ ê²½ìŸë ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.",
            "ìƒëª…ê³µí•™ ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ë‚œì¹˜ë³‘ ì¹˜ë£Œì™€ ì‹ ì•½ ê°œë°œì— ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì´ ì—´ë¦¬ê³  ìˆìŠµë‹ˆë‹¤.",
            "ê¸°í›„ ë³€í™”ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ì‹ ì¬ìƒ ì—ë„ˆì§€ ê¸°ìˆ  ê°œë°œì´ ì „ ì„¸ê³„ì ìœ¼ë¡œ í™œë°œíˆ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
            
            # Economy and society
            "í•œêµ­ ê²½ì œëŠ” ì œì¡°ì—…ê³¼ ì„œë¹„ìŠ¤ì—…ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ì—¬ ì„ ì§„êµ­ ëŒ€ì—´ì— í•©ë¥˜í–ˆìŠµë‹ˆë‹¤.",
            "ìŠ¤íƒ€íŠ¸ì—… ìƒíƒœê³„ì˜ í™œì„±í™”ë¡œ í˜ì‹ ì ì¸ ê¸°ì—…ë“¤ì´ ê³„ì†í•´ì„œ ë“±ì¥í•˜ê³  ìˆìœ¼ë©°, ìœ ë‹ˆì½˜ ê¸°ì—…ë„ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ê³ ë ¹í™” ì‚¬íšŒë¡œì˜ ì§„ì…ì— ë”°ë¼ ë³µì§€ ì •ì±…ê³¼ ì˜ë£Œ ì‹œìŠ¤í…œì˜ ê°œì„ ì´ ì¤‘ìš”í•œ ê³¼ì œë¡œ ë– ì˜¤ë¥´ê³  ìˆìŠµë‹ˆë‹¤.",
            
            # Literature and philosophy
            "í•œêµ­ ë¬¸í•™ì€ ê³ ì „ ë¬¸í•™ë¶€í„° í˜„ëŒ€ ë¬¸í•™ê¹Œì§€ ë‹¤ì–‘í•œ ì£¼ì œì™€ í˜•ì‹ìœ¼ë¡œ ì¸ê°„ì˜ ì‚¶ê³¼ ì •ì„œë¥¼ í‘œí˜„í•´ ì™”ìŠµë‹ˆë‹¤.",
            "ë™ì–‘ ì² í•™ì˜ ì „í†µ ì†ì—ì„œ í•œêµ­ì€ ìœ êµ, ë¶ˆêµ, ë„êµì˜ ì‚¬ìƒì„ ë…íŠ¹í•˜ê²Œ ìœµí•©í•˜ì—¬ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤.",
            "í˜„ëŒ€ í•œêµ­ ì‚¬íšŒëŠ” ì „í†µê³¼ í˜„ëŒ€, ë™ì–‘ê³¼ ì„œì–‘ì˜ ê°€ì¹˜ê´€ì´ ê³µì¡´í•˜ëŠ” ë‹¤ì›ì  ë¬¸í™”ë¥¼ í˜•ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        ]
        
        # Add more variations
        extended_samples = []
        for sample in samples:
            extended_samples.append(sample)
            # Add slight variations
            extended_samples.append(sample + " ì´ëŸ¬í•œ ë°œì „ì€ ì•ìœ¼ë¡œë„ ê³„ì†ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.")
            extended_samples.append("ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´, " + sample)
            
        print(f"âœ… Created {len(extended_samples)} high-quality samples")
        return extended_samples
        
    def prepare_training_data(self, texts: List[str], output_name: str) -> Path:
        """Convert texts to tokenized binary format"""
        print(f"\nğŸ”§ Preparing {output_name}...")
        
        # Tokenize all texts
        all_tokens = []
        
        for text in tqdm(texts, desc="Tokenizing"):
            if text and len(text.strip()) > 0:
                tokens = self.tokenizer.encode(text, language='ko')
                if tokens:
                    all_tokens.extend(tokens)
                    # Add a separator token
                    all_tokens.append(0)  # Use 0 as separator
                    
        # Convert to numpy array - use uint32 to avoid overflow
        # Filter out tokens that are out of range
        valid_tokens = [t for t in all_tokens if 0 <= t < 65536]
        tokens_array = np.array(valid_tokens, dtype=np.uint16)
        
        # Save
        output_path = self.data_dir / f"{output_name}.bin"
        tokens_array.tofile(output_path)
        
        print(f"âœ… Saved {len(tokens_array):,} tokens to {output_path}")
        return output_path
        
    def create_comprehensive_dataset(self):
        """Create a comprehensive Korean dataset from all available sources"""
        print("\nğŸ”§ Creating comprehensive Korean dataset...")
        
        all_texts = []
        
        # 1. Load datasets from Hugging Face
        datasets = {}
        datasets.update(self.load_klue_datasets())
        datasets.update(self.load_korquad_dataset())
        datasets.update(self.load_nsmc_dataset())
        datasets.update(self.load_korean_parallel_corpora())
        
        # 2. Add high-quality samples
        high_quality_samples = self.create_high_quality_samples()
        all_texts.extend(high_quality_samples * 10)  # Repeat for more data
        
        # 3. Combine all dataset texts
        for dataset_name, texts in datasets.items():
            print(f"\nAdding {len(texts)} texts from {dataset_name}")
            all_texts.extend(texts)
            
        # Remove duplicates and empty texts
        unique_texts = list(set(text.strip() for text in all_texts if text and len(text.strip()) > 0))
        print(f"\nTotal unique texts: {len(unique_texts)}")
        
        # Shuffle
        import random
        random.shuffle(unique_texts)
        
        # Split into train and validation
        split_idx = int(len(unique_texts) * 0.95)
        train_texts = unique_texts[:split_idx]
        val_texts = unique_texts[split_idx:]
        
        print(f"\nTrain texts: {len(train_texts)}")
        print(f"Validation texts: {len(val_texts)}")
        
        # Create binary files
        train_path = self.prepare_training_data(train_texts, "korean_hf_train")
        val_path = self.prepare_training_data(val_texts, "korean_hf_val")
        
        return train_path, val_path
        

def main():
    """Main function"""
    print("ğŸ‡°ğŸ‡· Korean Dataset Preparation from Hugging Face")
    print("=" * 60)
    
    preparer = KoreanHFDatasetPreparer()
    
    # Check if datasets library is available
    try:
        import datasets
        print("âœ… datasets library is available")
        
        # Create comprehensive dataset
        train_path, val_path = preparer.create_comprehensive_dataset()
        
        print("\n" + "="*60)
        print("âœ… Dataset preparation complete!")
        print(f"   Training data: {train_path}")
        print(f"   Validation data: {val_path}")
        
        # Update training script to use new data
        print("\nğŸš€ To train with this Korean dataset:")
        print("   1. Update data paths in train_brain_gpt.py")
        print("   2. Run: uv run brain_gpt/training/train_simple.py")
        
    except ImportError:
        print("âŒ datasets library not found!")
        print("\nTo use Hugging Face Korean datasets:")
        print("  1. Install: pip install datasets")
        print("  2. Run this script again")
        print("\nAlternatively, use the basic Korean dataset:")
        print("  uv run brain_gpt/training/download_korean_datasets.py")
        

if __name__ == "__main__":
    main()