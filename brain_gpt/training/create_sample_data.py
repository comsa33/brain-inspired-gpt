#!/usr/bin/env python3
"""
Create sample training data for Brain-Inspired GPT
Creates both English and Korean synthetic data for testing
"""

import os
import sys
import numpy as np
from pathlib import Path

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.multilingual_tokenizer import MultilingualBrainTokenizer


def create_sample_data():
    """Create sample training data files"""
    print("ğŸ”§ Creating sample training data...")
    
    # Initialize tokenizer
    tokenizer = MultilingualBrainTokenizer()
    
    # Create data directories
    data_dir = Path("data")
    en_dir = data_dir / "openwebtext"
    ko_dir = data_dir / "korean"
    
    en_dir.mkdir(parents=True, exist_ok=True)
    ko_dir.mkdir(parents=True, exist_ok=True)
    
    # English sample texts
    en_texts = [
        "The future of artificial intelligence is bright. Machine learning continues to advance rapidly.",
        "Deep neural networks have revolutionized computer vision and natural language processing.",
        "Python is the most popular programming language for data science and machine learning.",
        "Transformers have become the dominant architecture in modern NLP applications.",
        "The brain is an incredibly efficient information processing system.",
        "Neurons communicate through electrical and chemical signals.",
        "Sparse representations are key to biological intelligence.",
        "Energy efficiency is crucial for sustainable AI development.",
        "Large language models require significant computational resources.",
        "Research in brain-inspired computing is advancing rapidly.",
    ] * 100  # Repeat to create more data
    
    # Korean sample texts
    ko_texts = [
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ë§¤ìš° ë°ìŠµë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì»´í“¨í„° ë¹„ì „ê³¼ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì— í˜ëª…ì„ ì¼ìœ¼ì¼°ìŠµë‹ˆë‹¤.",
        "íŒŒì´ì¬ì€ ë°ì´í„° ê³¼í•™ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ê°€ì¥ ì¸ê¸° ìˆëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í˜„ëŒ€ ìì—°ì–´ ì²˜ë¦¬ì˜ ì£¼ìš” ì•„í‚¤í…ì²˜ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "ë‡ŒëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ íš¨ìœ¨ì ì¸ ì •ë³´ ì²˜ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
        "ë‰´ëŸ°ì€ ì „ê¸°ì , í™”í•™ì  ì‹ í˜¸ë¥¼ í†µí•´ ì†Œí†µí•©ë‹ˆë‹¤.",
        "í¬ì†Œ í‘œí˜„ì€ ìƒë¬¼í•™ì  ì§€ëŠ¥ì˜ í•µì‹¬ì…ë‹ˆë‹¤.",
        "ì—ë„ˆì§€ íš¨ìœ¨ì„±ì€ ì§€ì† ê°€ëŠ¥í•œ AI ê°œë°œì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.",
        "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ìƒë‹¹í•œ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤.",
        "ë‡Œì—ì„œ ì˜ê°ì„ ë°›ì€ ì»´í“¨íŒ… ì—°êµ¬ê°€ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    ] * 100  # Repeat to create more data
    
    # Tokenize English data
    print("ğŸ“ Tokenizing English data...")
    en_tokens = []
    for text in en_texts:
        tokens = tokenizer.encode(text, language='en')
        en_tokens.extend(tokens)
    
    # Save English data
    en_array = np.array(en_tokens, dtype=np.uint16)
    en_train_path = en_dir / "train.bin"
    en_array.tofile(en_train_path)
    print(f"âœ… Created {en_train_path} with {len(en_array):,} tokens")
    
    # Also create validation data (smaller)
    en_val_array = en_array[:10000]  # First 10k tokens for validation
    en_val_path = en_dir / "val.bin"
    en_val_array.tofile(en_val_path)
    print(f"âœ… Created {en_val_path} with {len(en_val_array):,} tokens")
    
    # Tokenize Korean data
    print("ğŸ“ Tokenizing Korean data...")
    ko_tokens = []
    for text in ko_texts:
        tokens = tokenizer.encode(text, language='ko')
        ko_tokens.extend(tokens)
    
    # Save Korean data
    ko_array = np.array(ko_tokens, dtype=np.uint16)
    ko_train_path = ko_dir / "train_korean.bin"
    ko_array.tofile(ko_train_path)
    print(f"âœ… Created {ko_train_path} with {len(ko_array):,} tokens")
    
    # Also create validation data
    ko_val_array = ko_array[:10000]  # First 10k tokens for validation
    ko_val_path = ko_dir / "val_korean.bin"
    ko_val_array.tofile(ko_val_path)
    print(f"âœ… Created {ko_val_path} with {len(ko_val_array):,} tokens")
    
    # Create metadata
    metadata = {
        "en_train_tokens": len(en_array),
        "en_val_tokens": len(en_val_array),
        "ko_train_tokens": len(ko_array),
        "ko_val_tokens": len(ko_val_array),
        "vocab_size": tokenizer.get_vocab_size(),
    }
    
    print("\nğŸ“Š Dataset Summary:")
    print(f"   English train: {metadata['en_train_tokens']:,} tokens")
    print(f"   English val: {metadata['en_val_tokens']:,} tokens")
    print(f"   Korean train: {metadata['ko_train_tokens']:,} tokens")
    print(f"   Korean val: {metadata['ko_val_tokens']:,} tokens")
    print(f"   Vocabulary size: {metadata['vocab_size']:,}")
    
    return metadata


def create_tiny_test_data():
    """Create tiny dataset for quick testing"""
    print("\nğŸ”§ Creating tiny test dataset...")
    
    # Initialize tokenizer
    tokenizer = MultilingualBrainTokenizer()
    
    # Create directories
    data_dir = Path("data/tiny")
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # Very simple data
    texts = [
        "Hello world! This is a test.",
        "ì•ˆë…•í•˜ì„¸ìš”! í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
        "AI is amazing. ì¸ê³µì§€ëŠ¥ì€ ë†€ëìŠµë‹ˆë‹¤.",
    ] * 10
    
    tokens = []
    for text in texts:
        tokens.extend(tokenizer.encode(text))
    
    # Save
    array = np.array(tokens, dtype=np.uint16)
    train_path = data_dir / "train.bin"
    array.tofile(train_path)
    
    print(f"âœ… Created {train_path} with {len(array):,} tokens")
    

if __name__ == "__main__":
    # Create sample data
    metadata = create_sample_data()
    
    # Also create tiny test data
    create_tiny_test_data()
    
    print("\nâœ… Sample data creation complete!")
    print("\nğŸš€ You can now run training with:")
    print("   uv run brain_gpt/training/train_brain_gpt.py")