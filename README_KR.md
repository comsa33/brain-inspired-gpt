<div align="center">

# ğŸ§  CortexGPT

**ì¸ê°„ ë‘ë‡Œì—ì„œ ì˜ê°ì„ ë°›ì€ ì‹¤ì‹œê°„ í•™ìŠµ ì–¸ì–´ ëª¨ë¸**

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Model Size](https://img.shields.io/badge/model-768D-purple.svg)
![Memory](https://img.shields.io/badge/memory-STMâ†’LTMâ†’Archive-orange.svg)

[English](README.md) | [í•œêµ­ì–´](#í•œêµ­ì–´)

</div>

## í•œêµ­ì–´

### ğŸ›ï¸ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "CortexGPT ëª¨ë¸"
        Input["ğŸ“¥ ì…ë ¥ ë ˆì´ì–´<br/>â€¢ ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €<br/>â€¢ í•œêµ­ì–´/ì˜ì–´ ì§€ì›<br/>â€¢ BPE ì¸ì½”ë”©"]
        
        Transformer["ğŸ¤– íŠ¸ëœìŠ¤í¬ë¨¸ ì½”ì–´<br/>â€¢ ë©€í‹° í—¤ë“œ ì–´í…ì…˜<br/>â€¢ í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬<br/>â€¢ ë ˆì´ì–´ ì •ê·œí™”<br/>â€¢ ì”ì°¨ ì—°ê²°"]
        
        subgraph "ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ"
            STM["ğŸ’­ STM (ë‹¨ê¸° ê¸°ì–µ)<br/>â€¢ ìš©ëŸ‰: 64<br/>â€¢ ë¹ ë¥¸ ì ‘ê·¼<br/>â€¢ ìµœê·¼ ìƒí˜¸ì‘ìš©"]
            LTM["ğŸ§  LTM (ì¥ê¸° ê¸°ì–µ)<br/>â€¢ ìš©ëŸ‰: 10,000<br/>â€¢ í†µí•©ëœ ì§€ì‹<br/>â€¢ ë¹ˆë²ˆí•œ íŒ¨í„´"]
            Archive["ğŸ“š Archive (ë³´ê´€ ë©”ëª¨ë¦¬)<br/>â€¢ ìš©ëŸ‰: 100,000<br/>â€¢ ì••ì¶• ì €ì¥<br/>â€¢ ë“œë¬¼ê²Œ ì‚¬ìš©ë˜ëŠ” ì§€ì‹"]
        end
        
        Learner["ğŸ“ ì‹¤ì‹œê°„ í•™ìŠµê¸°<br/>â€¢ ì˜¨ë¼ì¸ í•™ìŠµ<br/>â€¢ ë©”ëª¨ë¦¬ í†µí•©<br/>â€¢ ìê¸° í‰ê°€"]
        
        Output["ğŸ“¤ ì¶œë ¥ ë ˆì´ì–´<br/>â€¢ í† í° ìƒì„±<br/>â€¢ ì‹ ë¢°ë„ ì ìˆ˜<br/>â€¢ ì–¸ì–´ ê°ì§€"]
    end
    
    Input --> |"ì¸ì½”ë”©ëœ í† í°"| Transformer
    Transformer --> |"ì»¨í…ìŠ¤íŠ¸ ì €ì¥"| STM
    STM --> |"í†µí•©<br/>(ìì£¼ ì‚¬ìš©)"| LTM
    LTM --> |"ë³´ê´€<br/>(ë“œë¬¼ê²Œ ì‚¬ìš©)"| Archive
    STM --> |"í˜„ì¬ ì»¨í…ìŠ¤íŠ¸"| Learner
    LTM --> |"ê²€ìƒ‰ëœ ì§€ì‹"| Learner
    Learner --> |"ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸"| Transformer
    Transformer --> |"ì˜ˆì¸¡"| Output
    Learner -.-> |"ì—…ë°ì´íŠ¸"| STM
    Learner -.-> |"ì „ì†¡"| LTM
    
    style Input fill:#e6f3ff
    style Transformer fill:#e6f3ff
    style STM fill:#ffe6e6
    style LTM fill:#e6ffe6
    style Archive fill:#e6e6ff
    style Learner fill:#e6f3ff
    style Output fill:#e6f3ff
```

### ğŸŒŸ í•µì‹¬ íŠ¹ì§•

- **ì‹¤ì‹œê°„ í•™ìŠµ**: í›ˆë ¨/ì¶”ë¡  êµ¬ë¶„ ì—†ì´ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµ
- **ì¸ê°„ê³¼ ìœ ì‚¬í•œ ë©”ëª¨ë¦¬**: STM(ë‹¨ê¸°) â†’ LTM(ì¥ê¸°) â†’ Archive(ë³´ê´€) ì‹œìŠ¤í…œ
- **ìê¸° ê°œì„ **: ìŠ¤ìŠ¤ë¡œ í‰ê°€í•˜ê³  ê°œì„ í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: OOM ë°©ì§€ë¥¼ ìœ„í•œ ì ì‘í˜• ë°°ì¹˜ í¬ê¸° ì¡°ì •
- **ì²´í¬í¬ì¸íŠ¸ ì§€ì›**: ì¤‘ë‹¨ í›„ í›ˆë ¨ ì¬ê°œ ê°€ëŠ¥

### ğŸš€ ë¹ ë¥¸ ì‹œì‘

#### 1. ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/comsa33/cortexgpt.git
cd cortexgpt

# ëª¨ë“  ì˜ì¡´ì„± ì„¤ì¹˜
uv sync

# ë˜ëŠ” ëª¨ë‹ˆí„°ë§ ë„êµ¬ í¬í•¨ ì„¤ì¹˜
uv sync --extra monitoring
```

#### 2. ë°ëª¨ ë°ì´í„° ìƒì„±

```bash
# ë°ëª¨ í›ˆë ¨ ë°ì´í„° ìƒì„±
uv run scripts/data/create_demo_data.py
```

#### 3. ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸

```bash
# í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
uv run tests/demo_tokenizer.py

# ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸ (ê³¼ì í•© í…ŒìŠ¤íŠ¸)
uv run tests/test_overfit.py
```

#### 4. í›ˆë ¨

```bash
# ë¹ ë¥¸ ë°ëª¨ í›ˆë ¨ (ì‘ì€ ëª¨ë¸, ë¹ ë¦„)
uv run cortexgpt/training/train_realtime.py \
    --dataset demo \
    --dim 256 \
    --lr 1e-3 \
    --epochs 20

# wandbë¡œ ëª¨ë‹ˆí„°ë§
uv run cortexgpt/training/train_realtime.py \
    --dataset demo \
    --dim 512 \
    --wandb

# ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ (ì„¤ì • í›„)
uv run scripts/data/setup_datasets.py  # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„
uv run cortexgpt/training/train_realtime.py \
    --dataset klue \
    --batch-size 4 \
    --gradient-accumulation 8 \
    --epochs 50 \
    --wandb

# ì¤‘ë‹¨ëœ í›ˆë ¨ ì¬ê°œ
uv run cortexgpt/training/train_realtime.py \
    --dataset klue \
    --resume auto
```

#### 5. ë°ëª¨ ì‹¤í–‰

```bash
# ìµœì†Œ ìƒì„± ë°ëª¨
uv run scripts/demos/minimal_demo.py

# ì‹¤ì‹œê°„ í•™ìŠµ ë°ëª¨
uv run scripts/demos/learning_effect_demo.py

# ëŒ€í™”í˜• ì±„íŒ… ë°ëª¨
uv run scripts/demos/natural_language_demo.py
```

### ğŸ“– ìƒì„¸ ì‚¬ìš© ê°€ì´ë“œ

#### ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

```bash
# ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ ìƒì„±
uv run cortexgpt/inference/generate.py \
    --checkpoint checkpoints/best_model.pt \
    --prompt "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”" \
    --max-length 100

# í›ˆë ¨ëœ ëª¨ë¸ë¡œ ëŒ€í™”í˜• ì±„íŒ…
uv run cortexgpt/inference/chat.py \
    --checkpoint checkpoints/best_model.pt \
    --temperature 0.8
```

#### ì‹¤ì‹œê°„ í•™ìŠµ ë°ëª¨

ì‹¤ì‹œê°„ í•™ìŠµ ë°ëª¨ëŠ” CortexGPTê°€ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤:

```bash
# í•™ìŠµ íš¨ê³¼ ë°ëª¨ ì‹¤í–‰
uv run scripts/demos/learning_effect_demo.py
```

ì´ ë°ëª¨ëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
- ì§€ì‹ ì—†ì´ ì´ˆê¸° ì‘ë‹µ
- ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œë¶€í„° í•™ìŠµ
- í•™ìŠµ í›„ ê°œì„ ëœ ì‘ë‹µ
- ì‹œê°„ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ í†µí•©

#### ì»¤ìŠ¤í…€ í›ˆë ¨

ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ì˜ ê²½ìš°, JSONL íŒŒì¼ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”:

```json
{"text": "ì—¬ê¸°ì— í›ˆë ¨ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”"}
{"text": "ë˜ ë‹¤ë¥¸ í›ˆë ¨ ì˜ˆì œ"}
```

ê·¸ëŸ° ë‹¤ìŒ í›ˆë ¨:

```bash
# ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì¤€ë¹„
uv run cortexgpt/data/prepare_custom.py \
    --input your_data.jsonl \
    --output data/custom

# ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ í›ˆë ¨
uv run cortexgpt/training/train_realtime.py \
    --dataset custom \
    --vocab-size 30000 \
    --epochs 50
```

#### ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì„¤ì •

ë‹¤ì–‘í•œ ì‚¬ìš© ì‚¬ë¡€ì— ë§ê²Œ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ì„¸ìš”:

```bash
# ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•œ ì‘ì€ ë©”ëª¨ë¦¬
uv run cortexgpt/training/train_realtime.py \
    --stm-capacity 32 \
    --ltm-capacity 1000 \
    --archive-capacity 10000

# í”„ë¡œë•ì…˜ì„ ìœ„í•œ í° ë©”ëª¨ë¦¬
uv run cortexgpt/training/train_realtime.py \
    --stm-capacity 128 \
    --ltm-capacity 50000 \
    --archive-capacity 500000
```

#### API ì‚¬ìš©ë²•

```python
from cortexgpt import CortexGPT, MultilingualTokenizer

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
model = CortexGPT.from_pretrained("checkpoints/best_model.pt")
tokenizer = MultilingualTokenizer.from_pretrained("checkpoints/tokenizer.json")

# í…ìŠ¤íŠ¸ ìƒì„±
prompt = "ê¸°ê³„ í•™ìŠµì´ë€"
inputs = tokenizer.encode(prompt)
outputs = model.generate(inputs, max_length=100)
response = tokenizer.decode(outputs)
print(response)

# ì‹¤ì‹œê°„ í•™ìŠµ
from cortexgpt.learning import RealTimeLearner

learner = RealTimeLearner(model, tokenizer)
learner.start()  # ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì‹œì‘

# í•™ìŠµê³¼ í•¨ê»˜ ì¿¼ë¦¬ ì²˜ë¦¬
response, metadata = learner.process_query(
    "ê¸°ê³„ í•™ìŠµì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    learn=True
)
print(f"ì‘ë‹µ: {response}")
print(f"ì‹ ë¢°ë„: {metadata['confidence']}")
```

#### í›ˆë ¨ ëª¨ë‹ˆí„°ë§

Weights & Biasesë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„¸í•œ ëª¨ë‹ˆí„°ë§:

```bash
# ë¨¼ì € wandbì— ë¡œê·¸ì¸
wandb login

# ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ í›ˆë ¨
uv run cortexgpt/training/train_realtime.py \
    --dataset klue \
    --wandb \
    --wandb-project "cortexgpt-experiments" \
    --wandb-name "run-001"
```

ëª¨ë‹ˆí„°ë§ í•­ëª©:
- í›ˆë ¨/ê²€ì¦ ì†ì‹¤
- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
- ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì‚¬ìš©ëŸ‰
- ìƒ˜í”Œ ìƒì„±
- ì„±ëŠ¥ ë©”íŠ¸ë¦­

### ğŸŒ ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨í•˜ê¸°

#### 1ë‹¨ê³„: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

```bash
# ìƒ˜í”Œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (KLUE, ìœ„í‚¤í”¼ë””ì•„ ë“±)
uv run cortexgpt/data/download_datasets.py
```

ë‹¤ìŒ ë°ì´í„°ì…‹ì˜ ìƒ˜í”Œì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤:
- **KLUE**: í•œêµ­ì–´ ì–¸ì–´ ì´í•´ í‰ê°€ ë°ì´í„°ì…‹
- **í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„**: í•œêµ­ì–´ ë°±ê³¼ì‚¬ì „ ë¬¸ì„œ
- **ì˜ì–´ ìœ„í‚¤í”¼ë””ì•„**: ì˜ì–´ ë°±ê³¼ì‚¬ì „ ë¬¸ì„œ
- **OpenWebText**: ì›¹ í¬ë¡¤ë§ ë°ì´í„° (ìƒ˜í”Œ)

#### 2ë‹¨ê³„: ë°ì´í„°ì…‹ ì¤€ë¹„ (ì„ íƒì‚¬í•­)

í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ëŠ” JSONL íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ë§Œ, ë” ë¹ ë¥¸ ë¡œë”©ì„ ìœ„í•´ ì‚¬ì „ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
# ë‹¤ìš´ë¡œë“œí•œ ëª¨ë“  ë°ì´í„°ì…‹ ì¤€ë¹„
uv run cortexgpt/data/prepare_datasets.py
```

#### 3ë‹¨ê³„: ì‹¤ì œ ë°ì´í„°ë¡œ í›ˆë ¨

##### í•œêµ­ì–´ ë°ì´í„°ì…‹ (KLUE)
```bash
# KLUE ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨
uv run cortexgpt/training/train_realtime.py \
    --dataset klue \
    --dim 512 \
    --vocab-size 30000 \
    --batch-size 8 \
    --gradient-accumulation 4 \
    --lr 3e-4 \
    --epochs 10 \
    --wandb
```

##### ì˜ì–´ ë°ì´í„°ì…‹ (ìœ„í‚¤í”¼ë””ì•„)
```bash
# ì˜ì–´ ìœ„í‚¤í”¼ë””ì•„ë¡œ í›ˆë ¨
uv run cortexgpt/training/train_realtime.py \
    --dataset wikipedia \
    --dim 512 \
    --vocab-size 30000 \
    --batch-size 8 \
    --gradient-accumulation 4 \
    --lr 3e-4 \
    --epochs 10 \
    --wandb
```

##### í•œêµ­ì–´-ì˜ì–´ í˜¼í•© í›ˆë ¨
```bash
# ê²°í•©ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨
uv run cortexgpt/training/train_realtime.py \
    --dataset combined \
    --korean-ratio 0.4 \
    --dim 768 \
    --vocab-size 50000 \
    --batch-size 4 \
    --gradient-accumulation 8 \
    --lr 2e-4 \
    --epochs 20 \
    --wandb
```

#### 4ë‹¨ê³„: í›ˆë ¨ ì¬ê°œ

í›ˆë ¨ì´ ì¤‘ë‹¨ëœ ê²½ìš°:

```bash
# ìµœì‹  ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
uv run cortexgpt/training/train_realtime.py \
    --dataset klue \
    --resume auto \
    --wandb

# íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
uv run cortexgpt/training/train_realtime.py \
    --dataset klue \
    --resume checkpoints/realtime/model_best.pt \
    --wandb
```

#### í›ˆë ¨ íŒ

1. **ì‘ê²Œ ì‹œì‘í•˜ê¸°**: í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ `--dim 256`ê³¼ `--vocab-size 10000`ìœ¼ë¡œ ì‹œì‘
2. **ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**: OOM ë°œìƒ ì‹œ `--batch-size 2`ë¥¼ ì‚¬ìš©í•˜ê³  `--gradient-accumulation` ì¦ê°€
3. **í•™ìŠµë¥ **: ì‘ì€ ëª¨ë¸ì€ `1e-3`, í° ëª¨ë¸ì€ `3e-4`ë¡œ ì‹œì‘
4. **ì–´íœ˜ í¬ê¸°**: 
   - í•œêµ­ì–´ë§Œ: 20,000-30,000
   - ì˜ì–´ë§Œ: 30,000-40,000
   - í˜¼í•©: 40,000-50,000

### ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹ | ì–¸ì–´ | ì„¤ëª… |
|---------|------|------|
| `demo` | í˜¼í•© | ì‘ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ê¸°ë³¸ê°’) |
| `klue` | í•œêµ­ì–´ | í•œêµ­ì–´ ì–¸ì–´ ì´í•´ í‰ê°€ |
| `wikipedia` | ì˜ì–´ | ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ |
| `korean_wiki` | í•œêµ­ì–´ | í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ |
| `openwebtext` | ì˜ì–´ | GPT-2 í›ˆë ¨ìš© ì›¹ í…ìŠ¤íŠ¸ |
| `combined` | í˜¼í•© | ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì¡°í•© |

### ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
my-efficient-gpt/
â”œâ”€â”€ cortexgpt/              # ë©”ì¸ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ models/            # ëª¨ë¸ ì•„í‚¤í…ì²˜
â”‚   â”œâ”€â”€ learning/          # ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ tokenization/      # ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì €
â”‚   â”œâ”€â”€ data/             # ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ training/         # í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data/             # ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ demos/            # ë°ëª¨ ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ tests/                # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ docs/                 # ë¬¸ì„œ
â””â”€â”€ data/                 # í›ˆë ¨ ë°ì´í„°
```

### ğŸ’¡ ì‘ë™ ì›ë¦¬

#### ë©”ëª¨ë¦¬ íë¦„
```
ìƒˆë¡œìš´ ì…ë ¥ â†’ STM (ë¹ ë¥¸ ì ‘ê·¼)
     â†“ (ìì£¼ ì‚¬ìš©)
    LTM (í†µí•©ëœ ì§€ì‹)
     â†“ (ì˜¤ë˜ ë¯¸ì‚¬ìš©)
   Archive (ì••ì¶• ì €ì¥)
```

#### í•™ìŠµ ê³¼ì •
1. **ì²« ì§ˆë¬¸**: "ì•„ì§ í•™ìŠµí•˜ì§€ ëª»í•œ ë‚´ìš©ì…ë‹ˆë‹¤"
2. **í•™ìŠµ í›„**: ì •í™•í•œ ë‹µë³€ ì œê³µ
3. **ë°˜ë³µ ì‹œ**: ì‹ ë¢°ë„ ì¦ê°€ (0.6 â†’ 0.9 â†’ 1.0)

### ğŸ“ˆ í›ˆë ¨ ì˜µì…˜

```bash
# ëª¨ë¸ ì•„í‚¤í…ì²˜
--dim               # íˆë“  ì°¨ì› (256/512/768, ê¸°ë³¸ê°’: 768)
--vocab-size        # í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸° (ê¸°ë³¸ê°’: 50000)

# í›ˆë ¨ íŒŒë¼ë¯¸í„°
--batch-size        # ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)
--gradient-accumulation  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ (ê¸°ë³¸ê°’: 4)
--epochs           # ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: 10)
--lr              # í•™ìŠµë¥  (ê¸°ë³¸ê°’: 3e-4)

# ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
--stm-capacity     # ë‹¨ê¸° ê¸°ì–µ ìš©ëŸ‰ (ê¸°ë³¸ê°’: 64)
--ltm-capacity     # ì¥ê¸° ê¸°ì–µ ìš©ëŸ‰ (ê¸°ë³¸ê°’: 10000)
--archive-capacity # ë³´ê´€ ìš©ëŸ‰ (ê¸°ë³¸ê°’: 100000)

# ëª¨ë‹ˆí„°ë§ ë° ì²´í¬í¬ì¸íŒ…
--wandb           # Weights & Biases ë¡œê¹… í™œì„±í™”
--wandb-project   # W&B í”„ë¡œì íŠ¸ ì´ë¦„
--checkpoint-dir  # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
--resume         # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ (auto/ê²½ë¡œ)
```

### ğŸš€ ê¶Œì¥ í›ˆë ¨ ì„¤ì •

#### í…ŒìŠ¤íŠ¸ ë° ê°œë°œ
```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‘ì€ ëª¨ë¸
--dim 256 --lr 1e-3 --batch-size 4 --epochs 20
```

#### ë°ëª¨ í›ˆë ¨
```bash
# ë°ëª¨ë¥¼ ìœ„í•œ ì¤‘ê°„ ëª¨ë¸
--dim 512 --lr 5e-4 --batch-size 8 --gradient-accumulation 4
```

#### í”„ë¡œë•ì…˜ í›ˆë ¨
```bash
# ì‹¤ì œ í›ˆë ¨ì„ ìœ„í•œ í° ëª¨ë¸
--dim 768 --lr 3e-4 --batch-size 4 --gradient-accumulation 8 --wandb
```

### ğŸ”¬ ì—°êµ¬ ë° ê°œë°œ

CortexGPTëŠ” ì—¬ëŸ¬ ì‹ ê²½ê³¼í•™ ê°œë…ì„ êµ¬í˜„í•©ë‹ˆë‹¤:

- **í—¤ë¹„ì•ˆ í•™ìŠµ**: "í•¨ê»˜ ë°œí™”í•˜ëŠ” ë‰´ëŸ°ì€ í•¨ê»˜ ì—°ê²°ëœë‹¤"
- **ë©”ëª¨ë¦¬ í†µí•©**: STMì—ì„œ LTMìœ¼ë¡œì˜ ì ì§„ì  ì „ì´
- **ì„ íƒì  ì£¼ì˜**: ê´€ë ¨ ì •ë³´ì— ì§‘ì¤‘
- **ì§€ì†ì  í•™ìŠµ**: ìŠì§€ ì•Šê³  ìƒˆë¡œìš´ ì‘ì—… í•™ìŠµ

### ğŸ“ ì¸ìš©

```bibtex
@software{cortexgpt2024,
  author = {Ruo Lee},
  title = {CortexGPT: Real-time Learning Language Model},
  year = {2025},
  email = {comsa333@gmail.com}
}
```

### ğŸ“„ ë¼ì´ì„ ìŠ¤

MIT ë¼ì´ì„ ìŠ¤ - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

Made with â¤ï¸ by Ruo Lee