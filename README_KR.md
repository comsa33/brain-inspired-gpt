# ğŸ§  Brain-Inspired GPT

<div align="center">

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8+-76b900.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Model Size](https://img.shields.io/badge/model-60M--2.5B-purple.svg)
![Sparsity](https://img.shields.io/badge/sparsity-95%25-orange.svg)

[English](README.md) | [í•œêµ­ì–´](#korean)

</div>

## ğŸŒŸ ê°œìš”

Brain-Inspired GPTëŠ” ì¸ê°„ ë‡Œì˜ sparse activation íŒ¨í„´ì„ ëª¨ë°©í•˜ì—¬ 95% sparsityë¥¼ ë‹¬ì„±í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ 5%ë§Œ í™œì„±í™”í•˜ë©´ì„œë„ ê¸°ì¡´ dense ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ ì—°êµ¬í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤. íŠ¹íˆ edge deploymentì™€ íš¨ìœ¨ì ì¸ AI ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ëŠ¥ì„±ì„ íƒêµ¬í•©ë‹ˆë‹¤.

### âœ¨ ì£¼ìš” íŠ¹ì§•

- **ğŸ§  Brain-like Sparsity**: ìƒë¬¼í•™ì  ì‹ ê²½ë§ì˜ 95% sparse activation êµ¬í˜„
- **âš¡ RTX 3090 ìµœì í™”**: 2:4 structured sparsityë¥¼ ìœ„í•œ custom CUDA kernel
- **ğŸ›ï¸ Cortical Columns**: Neocortexì˜ columnar organizationì„ ëª¨ë°©í•œ ëª¨ë“ˆì‹ ì•„í‚¤í…ì²˜
- **ğŸŒ¿ Dendritic Attention**: ìƒë¬¼í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•œ attention mechanism
- **ğŸŒ ë‹¤êµ­ì–´ ì§€ì›**: í™•ì¥ ê°€ëŠ¥í•œ tokenizerë¡œ í•œêµ­ì–´ + ì˜ì–´ ì§€ì›
- **ğŸ“ˆ Developmental Learning**: Curriculum learningì„ í†µí•œ ì ì§„ì  complexity ì¦ê°€

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.11+
- CUDA 11.8+ ì§€ì› NVIDIA GPU (RTX 3090 ê¶Œì¥)
- ì „ì²´ ëª¨ë¸ìš© 24GB+ VRAM, ì†Œí˜• ëª¨ë¸ìš© 8GB+

### uvë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜

ì´ í”„ë¡œì íŠ¸ëŠ” ë¹ ë¥´ê³  ì•ˆì •ì ì¸ Python íŒ¨í‚¤ì§€ ê´€ë¦¬ë¥¼ ìœ„í•´ [uv](https://github.com/astral-sh/uv)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```bash
# uvê°€ ì—†ë‹¤ë©´ ë¨¼ì € ì„¤ì¹˜
curl -LsSf https://astral.sh/uv/install.sh | sh

# ì €ì¥ì†Œ ë³µì œ
git clone https://github.com/comsa33/brain-inspired-gpt.git
cd brain-inspired-gpt

# ëª¨ë“  ì¢…ì†ì„± ì„¤ì¹˜ (ìë™ìœ¼ë¡œ venv ìƒì„±)
uv sync

# ë¹ ë¥¸ ê²€ì¦
uv run validate_brain_gpt.py

# ëŒ€í™”í˜• ë°ëª¨ ì‹¤í–‰
uv run brain_gpt/quickstart.py
```

**ì™œ uvì¸ê°€?**
- âš¡ pipë³´ë‹¤ 10-100ë°° ë¹ ë¦„
- ğŸ”’ lockfileë¡œ ìë™ ì¢…ì†ì„± í•´ê²°
- ğŸ¯ ëª¨ë“  ì¢…ì†ì„±ì„ ë‹¨ì¼ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜
- ğŸ”§ ë‚´ì¥ëœ ê°€ìƒ í™˜ê²½ ê´€ë¦¬

## ğŸ“Š ëª¨ë¸ ì•„í‚¤í…ì²˜

| Model | Layers | Hidden | Heads | Total Params | Effective (5%) | VRAM Usage |
|------|--------|------|------|---------------|-----------|-------------|
| Small | 6 | 512 | 8 | 60.1M | 3.0M | ~0.5GB |
| Medium | 12 | 1024 | 16 | 221.8M | 11.1M | ~2.8GB |
| Large | 24 | 1536 | 24 | 495.2M | 24.8M | ~6.2GB |
| XLarge | 48 | 2048 | 32 | 2.59B | 130M | ~24GB |

## ğŸ¯ ì‚¬ìš©ë²•

### ëª¨ë¸ í•™ìŠµ

```bash
# í…ŒìŠ¤íŠ¸ìš© ì†Œí˜• ëª¨ë¸
uv run brain_gpt/training/train_simple.py

# í•œêµ­ì–´ ì–¸ì–´ ëª¨ë¸
uv run brain_gpt/training/train_korean.py

# RTX 3090 ìµœì í™” í•™ìŠµ
uv run brain_gpt/training/train_brain_gpt_3090.py

# ì „ì²´ ëª¨ë¸ (24GB+ VRAM í•„ìš”)
uv run brain_gpt/training/train_brain_gpt.py
```

### í…ìŠ¤íŠ¸ ìƒì„±

```python
from brain_gpt import BrainGPT, BrainGPTConfig
from brain_gpt.core.multilingual_tokenizer import MultilingualBrainTokenizer

# ëª¨ë¸ ë¡œë“œ
config = BrainGPTConfig()
model = BrainGPT.from_pretrained("checkpoints/brain_gpt_3090_best.pt")
tokenizer = MultilingualBrainTokenizer()

# ì˜ì–´ í…ìŠ¤íŠ¸ ìƒì„±
prompt = "The future of AI is"
tokens = tokenizer.encode(prompt)
output = model.generate(tokens, max_new_tokens=50, temperature=0.8)
print(tokenizer.decode(output))

# í•œêµ­ì–´ ìƒì„±
prompt_ko = "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ”"
tokens_ko = tokenizer.encode(prompt_ko, language='ko')
output_ko = model.generate(tokens_ko, max_new_tokens=50, temperature=0.8)
print(tokenizer.decode(output_ko))
```

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
brain-inspired-gpt/
â”œâ”€â”€ brain_gpt/
â”‚   â”œâ”€â”€ core/                 # Core model implementation
â”‚   â”‚   â”œâ”€â”€ model_brain.py         # Main Brain-Inspired GPT model
â”‚   â”‚   â”œâ”€â”€ sparse_layers.py       # 95% sparse layers with CUDA
â”‚   â”‚   â”œâ”€â”€ attention_dendritic.py # Dendritic attention mechanism
â”‚   â”‚   â””â”€â”€ multilingual_tokenizer.py # Korean + English tokenizer
â”‚   â”œâ”€â”€ training/             # Training scripts
â”‚   â”‚   â”œâ”€â”€ train_simple.py        # Quick demo training
â”‚   â”‚   â”œâ”€â”€ train_korean.py        # Korean language training
â”‚   â”‚   â””â”€â”€ train_brain_gpt_3090.py # RTX 3090 optimized
â”‚   â”œâ”€â”€ tests/                # Test suites
â”‚   â””â”€â”€ docs/                 # Documentation
â”œâ”€â”€ data/                     # Datasets
â”‚   â”œâ”€â”€ korean_hf/               # Korean datasets from HuggingFace
â”‚   â””â”€â”€ openwebtext/             # English datasets
â”œâ”€â”€ checkpoints/              # Saved models
â”œâ”€â”€ pyproject.toml            # Project configuration
â””â”€â”€ uv.lock                   # Locked dependencies
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run brain_gpt/tests/run_all_tests.py

# íŠ¹ì • í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
uv run brain_gpt/tests/comprehensive_test.py

# ëª¨ë¸ ê¸°ëŠ¥ ê²€ì¦
uv run validate_brain_gpt.py
```

## ğŸ“š ë¬¸ì„œ

ëª¨ë“  í•„ìˆ˜ ì •ë³´ëŠ” ì´ READMEì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŠ¹ì • ì£¼ì œëŠ” ìœ„ì˜ ê´€ë ¨ ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸŒ í•œêµ­ì–´ ì§€ì›

Brain-Inspired GPTëŠ” ë‹¤ìŒì„ í¬í•¨í•œ ì™„ì „í•œ í•œêµ­ì–´ ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤:
- ì»¤ìŠ¤í…€ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €
- KLUE, KorQuAD, ë³‘ë ¬ ë§ë­‰ì¹˜ì˜ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹
- í•œêµ­ì–´ íŠ¹í™” í•™ìŠµ êµ¬ì„±

### í•œêµ­ì–´ ë°ì´í„°ì…‹ í†µê³„
- í•™ìŠµ: 4,660ë§Œ í† í° (95ë§Œ ê³ ìœ  í…ìŠ¤íŠ¸)
- ê²€ì¦: 240ë§Œ í† í° (5ë§Œ ê³ ìœ  í…ìŠ¤íŠ¸)
- ì¶œì²˜: KLUE, KorQuAD, í•œ-ì˜ ë³‘ë ¬ ë§ë­‰ì¹˜

## ğŸ”¬ ê¸°ì¡´ Transformerì™€ì˜ ì°¨ë³„ì 

### 1. Sparse Activation Pattern
- **ê¸°ì¡´ Transformer**: ëª¨ë“  ë‰´ëŸ°ì´ denseí•˜ê²Œ í™œì„±í™” (100% activation)
- **Brain-Inspired GPT**: ê° forward passì—ì„œ 5%ë§Œ í™œì„±í™” (95% sparsity)
- **êµ¬í˜„ ë°©ì‹**: Magnitude-based pruningê³¼ structured sparsity (2:4 pattern for RTX GPUs)

### 2. Cortical Column Architecture
- **ê¸°ì¡´ Transformer**: Flat layer structure with uniform processing
- **Brain-Inspired GPT**: Modular cortical columns (32 columns Ã— 64 neurons)
- **íŠ¹ì§•**: Lateral inhibitionì„ í†µí•œ column ê°„ competition, local processing ê°•í™”

### 3. Dendritic Attention Mechanism
- **ê¸°ì¡´ Transformer**: Single attention pathway per head
- **Brain-Inspired GPT**: Multiple dendrites per neuron (4 dendrites default)
- **íš¨ê³¼**: Context-dependent sparse routing, biologically plausible gradient flow

### 4. Developmental Stage Training
- **ê¸°ì¡´ Transformer**: Fixed architecture throughout training
- **Brain-Inspired GPT**: 5-stage progressive growth mimicking human development
- **Stage êµ¬ì„±**:
  - Stage 1: Basic pattern recognition (2 layers)
  - Stage 2: Simple language understanding (4 layers)
  - Stage 3: Complex reasoning (8 layers)
  - Stage 4: Abstract thinking (12 layers)
  - Stage 5: Full capacity (all layers)

### 5. Early Exit Mechanism
- **ê¸°ì¡´ Transformer**: ëª¨ë“  layerë¥¼ ê±°ì³ì•¼ ì¶œë ¥ ìƒì„±
- **Brain-Inspired GPT**: Confidence ê¸°ë°˜ early exit (í‰ê·  40% layerë§Œ ì‚¬ìš©)
- **ì´ì **: Dynamic computation allocation, energy efficiency

## ğŸ’¡ ì£¼ìš” ì—°êµ¬ ë‚´ìš©

### 1. Extreme Sparsity (95%)
- ì „ì²´ ë‰´ëŸ°ì˜ 5%ë§Œ ë™ì‹œ í™œì„±í™”
- ìƒë¬¼í•™ì  ë‡Œì˜ sparse coding ì›ë¦¬ ì ìš©
- 20ë°° íŒŒë¼ë¯¸í„° ê°ì†Œë¥¼ í†µí•œ íš¨ìœ¨ì„± ê²€ì¦

### 2. Cortical Columns
- Neocortexì˜ modular processing unit êµ¬í˜„
- 32 columns Ã— 64 neurons êµ¬ì„±
- Lateral inhibitionì„ í†µí•œ competition mechanism

### 3. Dendritic Attention
- ë‰´ëŸ°ë‹¹ multiple dendrites êµ¬í˜„
- Sparse, context-dependent routing
- Biologically plausible credit assignment

### 4. Developmental Learning
- 5ë‹¨ê³„ curriculum learning ì ìš©
- Progressive architectural growth
- Human cognitive development ëª¨ë°© ì‹œë„

## ğŸ› ï¸ ê³ ê¸‰ êµ¬ì„±

### ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì„±

```python
from brain_gpt import BrainGPTConfig

config = BrainGPTConfig()
config.n_layer = 12
config.n_head = 16
config.n_embd = 1024
config.sparsity_base = 0.95  # 95% í¬ì†Œì„±
config.n_cortical_columns = 32
config.column_size = 32  # 32 * 32 = 1024
config.gradient_checkpointing = True  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´
```

### ì»¤ìŠ¤í…€ ë°ì´í„°ë¡œ í•™ìŠµ

```bash
# ë°ì´í„°ì…‹ ì¤€ë¹„
uv run brain_gpt/data/openwebtext/prepare.py --input your_data.txt

# ì»¤ìŠ¤í…€ êµ¬ì„±ìœ¼ë¡œ í•™ìŠµ
uv run brain_gpt/training/train_brain_gpt_3090.py \
  --data-path data/your_dataset \
  --config-path configs/your_config.json \
  --batch-size 4 \
  --learning-rate 3e-4
```

## ğŸ“ˆ ì„±ëŠ¥

### ë²¤ì¹˜ë§ˆí¬ (RTX 3090)

| ì§€í‘œ | Small (60M) | Medium (221M) | Large (495M) |
|------|-------------|---------------|--------------|
| í¼í”Œë ‰ì‹œí‹° | 32.4 | 24.7 | 19.8 |
| í•™ìŠµ ì†ë„ | 12K tok/s | 8K tok/s | 4K tok/s |
| ì¶”ë¡  ì†ë„ | 120 tok/s | 85 tok/s | 45 tok/s |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 0.5GB | 2.8GB | 6.2GB |

### ì˜ˆìƒ íš¨ìœ¨ì„± (ì—°êµ¬ ëª©í‘œ)
- Dense ëª¨ë¸ ëŒ€ë¹„ **95% ì ì€ active parameters**
- Sparse kernel í™œìš© ì‹œ **10-20ë°° ë¹ ë¥¸ inference** ëª©í‘œ
- Edge deploymentë¥¼ ìœ„í•œ **5-10ë°° memory ê°ì†Œ** ê¸°ëŒ€

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! ìì„¸í•œ ë‚´ìš©ì€ [ê¸°ì—¬ ê°€ì´ë“œ](CONTRIBUTING.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ í™˜ê²½ ë³µì œ ë° ì„¤ì •
git clone https://github.com/comsa33/brain-inspired-gpt.git
cd brain-inspired-gpt

# ê°œë°œ ë„êµ¬ë¥¼ í¬í•¨í•œ ëª¨ë“  ì¢…ì†ì„± ì„¤ì¹˜
uv sync --all-extras

# PR ì œì¶œ ì „ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run pytest
uv run black .
uv run isort .
```

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤ - ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ™ Acknowledgments

- Cortical columnsì™€ sparse coding ê´€ë ¨ neuroscience ì—°êµ¬ì—ì„œ ì˜ê°ì„ ë°›ìŒ
- PyTorchì™€ Tritonì„ í™œìš©í•œ efficient sparse operations êµ¬í˜„
- KLUE ë° KorQuAD í”„ë¡œì íŠ¸ì˜ í•œêµ­ì–´ ë°ì´í„°ì…‹ í™œìš©

## ğŸ“® ì—°ë½ì²˜

- ì´ìŠˆ: [GitHub Issues](https://github.com/comsa33/brain-inspired-gpt/issues)
- ì´ë©”ì¼: comsa333@gmail.com

---

<div align="center">
Ruo Leeê°€ â¤ï¸ë¥¼ ë‹´ì•„ ë§Œë“¦
</div>